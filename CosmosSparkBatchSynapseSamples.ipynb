{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Intro**\r\n",
        "\r\n",
        "This notebook is based on the official Cosmos Spark v3 [sample notebook](https://aka.ms/azure-cosmos-spark-3-sample-nyc-taxi-data) which was designed for Cosmos Spark connector using Databricks Spark.\r\n",
        "In this notebook we illustrate new Cosmos Spark v3 connector features and show how easily it can be ported to be used with Synapse Spark."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Secrets**\n",
        "\n",
        "The secrets below  like the Cosmos account and Key are retrieved from a KeyVault secret scope.\n",
        "\n",
        "Here is the [example](https://docs.microsoft.com/en-us/answers/questions/445496/access-secret-from-vault-using-synapse-pyspark-not.html) on how to implement this with Synapse and KeyVault \n",
        "If you don't want to use secrets at all you can of course also just assign the values in clear-text below - but for obvious reasons we recommend the usage of secrets.\n",
        "\n",
        "\n",
        "Alternatively you can also use Synapse Linked service ```\"spark.synapse.linkedService\"``` for connectivity.\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b08b1d81-028c-4dba-ac24-b23ceebc9e38"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosmosEndpoint = TokenLibrary.getSecret('ssm-synapse-kv', 'cosmos-accountEndpoint')\n",
        "cosmosMasterKey = TokenLibrary.getSecret('ssm-synapse-kv', 'cosmos-accountKey')\n",
        "\n",
        "#print('Cosmos account URL:',cosmosEndpoint)\n",
        "#print('Cosmos account Key:',cosmosMasterKey)\n",
        "\n",
        "cosmosLinkedService = \"CosmosSampleDatabase\"\n",
        "cosmosDatabase = \"SampleDatabase\"\n",
        "\n",
        "print('Setup Cosmos Account vars')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "74303a6e-555c-44e2-8255-c8aea71caf03"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparation - creating the Cosmos DB container to ingest the data into**\n",
        "\n",
        "Configure the Catalog API to be used"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2ce582cf-5601-4827-8680-89bb3c0fa846"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint)\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", cosmosMasterKey)\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.views.repositoryPath\", \"/viewDefinitions\" + str(uuid.uuid4()))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b6d2168c-f06a-4445-8bd1-f7e1d0a902cb"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternative Catalog API configuration using Synapse Linked Service"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\r\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\r\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.synapse.linkedService\", cosmosLinkedService)\r\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.views.repositoryPath\", \"/viewDefinitions\" + str(uuid.uuid4()))\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the Catalog SQL commands to create the new DB/containers with a throughput of up-to 100,000 RU (Autoscale - so 10,000 - 100,000 RU based on scale) and only system properties (like /id) being indexed for one nad ALL properties indexed for another. We will also create additional container that will be used to store metadata for the global throughput control"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "24ef8b89-25ee-4477-a4e7-208fabab15fe"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql \n",
        "CREATE DATABASE IF NOT EXISTS cosmosCatalog.SampleDatabase;\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecords\n",
        "USING cosmos.oltp\n",
        "TBLPROPERTIES(partitionKeyPath = '/id', autoScaleMaxThroughput = '100000', indexingPolicy = 'AllProperties');\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecordsCFSink\n",
        "USING cosmos.oltp\n",
        "TBLPROPERTIES(partitionKeyPath = '/id', autoScaleMaxThroughput = '100000', indexingPolicy = 'OnlySystemProperties');\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecordsMAXThroughput\n",
        "USING cosmos.oltp\n",
        "TBLPROPERTIES(partitionKeyPath = '/id', autoScaleMaxThroughput = '100000', indexingPolicy = 'OnlySystemProperties');\n",
        "\n",
        "/* NOTE: It is important to enable TTL (can be off/-1 by default) on the throughput control container */\n",
        "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.ThroughputControl\n",
        "USING cosmos.oltp\n",
        "OPTIONS(spark.cosmos.database = 'SampleDatabase')\n",
        "TBLPROPERTIES(partitionKeyPath = '/groupId', autoScaleMaxThroughput = '4000', indexingPolicy = 'AllProperties', defaultTtlInSeconds = '-1');"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c61c309d-7a20-4488-a4e9-59bbaae19c96"
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**\r\n",
        "\r\n",
        "If you plan to test Reading using OLAP option from Cosmos Analytical Store - you will need to pre-create target containers manually (Catalog API does not yet support options for Analytical Store)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql \r\n",
        "CREATE DATABASE IF NOT EXISTS cosmosCatalog.SampleDatabase;\r\n",
        "\r\n",
        "CREATE TABLE IF NOT EXISTS cosmosCatalog.SampleDatabase.ThroughputControl\r\n",
        "USING cosmos.oltp\r\n",
        "OPTIONS(spark.cosmos.database = 'SampleDatabase')\r\n",
        "TBLPROPERTIES(partitionKeyPath = '/groupId', autoScaleMaxThroughput = '4000', indexingPolicy = 'AllProperties', defaultTtlInSeconds = '-1');"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparation - loading data source \"[NYC Taxi & Limousine Commission - green taxi trip records](https://azure.microsoft.com/services/open-datasets/catalog/nyc-taxi-limousine-commission-green-taxi-trip-records/)\"**\n",
        "\n",
        "The green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. This data set has over 80 million records (>8 GB) of data and is available via a publicly accessible Azure Blob Storage Account located in the East-US Azure region."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f719dedb-d782-4b54-9076-6d29da946950"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import time\n",
        "import uuid\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType, LongType\n",
        "\n",
        "print(\"Starting preparation: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
        "# Azure storage access info\n",
        "blob_account_name = \"azureopendatastorage\"\n",
        "blob_container_name = \"nyctlc\"\n",
        "blob_relative_path = \"green\"\n",
        "blob_sas_token = r\"\"\n",
        "# Allow SPARK to read from Blob remotely\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
        "spark.conf.set(\n",
        "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
        "  blob_sas_token)\n",
        "print('Remote blob path: ' + wasbs_path)\n",
        "# SPARK read parquet, note that it won't load any data yet by now\n",
        "# NOTE - if you want to experiment with larger dataset sizes - consider switching to Option B (commenting code \n",
        "# for Option A/uncommenting code for option B) the lines below or increase the value passed into the \n",
        "# limit function restricting the dataset size below\n",
        "\n",
        "#------------------------------------------------------------------------------------\n",
        "# Option A - with limited dataset size\n",
        "#------------------------------------------------------------------------------------\n",
        "df_rawInputWithoutLimit = spark.read.parquet(wasbs_path)\n",
        "partitionCount = df_rawInputWithoutLimit.rdd.getNumPartitions()\n",
        "df_rawInput = df_rawInputWithoutLimit.limit(5_000_000).repartition(partitionCount)\n",
        "df_rawInput.persist()\n",
        "\n",
        "#------------------------------------------------------------------------------------\n",
        "# Option B - entire dataset\n",
        "#------------------------------------------------------------------------------------\n",
        "#df_rawInput = spark.read.parquet(wasbs_path)\n",
        "#df_rawInput.persist()\n",
        "\n",
        "# Adding an id column with unique values\n",
        "uuidUdf= udf(lambda : str(uuid.uuid4()),StringType())\n",
        "nowUdf= udf(lambda : int(time.time() * 1000),LongType())\n",
        "df_input_withId = df_rawInput \\\n",
        "  .withColumn(\"id\", uuidUdf()) \\\n",
        "  .withColumn(\"insertedAt\", nowUdf()) \\\n",
        "\n",
        "print('Register the DataFrame as a SQL temporary view: source')\n",
        "df_input_withId.createOrReplaceTempView('source')\n",
        "print(\"Finished preparation: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "080c50a8-2d33-4f0f-9fba-c5b4212b7630"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Sample - ingesting the NYC Green Taxi data into Cosmos DB**\n",
        "\n",
        "By setting the target throughput threshold to 0.90 (90%) we reduce throttling but still allow the ingestion to consume most of the provisioned throughput. For scenarios where ingestion should only take a smaller subset of the available throughput this threshold can be reduced accordingly.\n",
        "\n",
        "Below example will ingest data with default Index policy (ALL)."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2805de61-e652-4be6-a3e5-b084cd848dc9"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import datetime\n",
        "\n",
        "print(\"Starting ingestion: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
        "\n",
        "writeCfg = {\n",
        "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n",
        "  \"spark.cosmos.accountKey\": cosmosMasterKey,\n",
        "  \"spark.cosmos.database\": cosmosDatabase,\n",
        "  \"spark.cosmos.container\": \"GreenTaxiRecords\",\n",
        "  \"spark.cosmos.write.strategy\": \"ItemOverwrite\",\n",
        "  \"spark.cosmos.write.bulk.enabled\": \"true\",\n",
        "  \"spark.cosmos.throughputControl.enabled\": \"true\",\n",
        "  \"spark.cosmos.throughputControl.name\": \"NYCGreenTaxiDataIngestion\",\n",
        "  \"spark.cosmos.throughputControl.targetThroughputThreshold\": \"0.90\",\n",
        "  \"spark.cosmos.throughputControl.globalControl.database\": \"SampleDatabase\",\n",
        "  \"spark.cosmos.throughputControl.globalControl.container\": \"ThroughputControl\",\n",
        "}\n",
        "\n",
        "df_NYCGreenTaxi_Input = spark.sql('SELECT * FROM source')\n",
        "\n",
        "df_NYCGreenTaxi_Input \\\n",
        "  .write \\\n",
        "  .format(\"cosmos.oltp\") \\\n",
        "  .mode(\"Append\") \\\n",
        "  .options(**writeCfg) \\\n",
        "  .save()\n",
        "\n",
        "print(\"Finished ingestion: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "56fc9432-df8b-456d-97c1-498c90938ed3"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternative write format using Synapse LinkedService option.\r\n",
        "\r\n",
        "Below example will ingest the same dataset to a conainer with IndexPolicy \"ID only\"."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\r\n",
        "import datetime\r\n",
        "\r\n",
        "print(\"Starting ingestion via Linked Service: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "\r\n",
        "df_NYCGreenTaxi_Input = spark.sql('SELECT * FROM source')\r\n",
        "\r\n",
        "SlinkwriteCfg = {\r\n",
        "  \"spark.synapse.linkedService\": cosmosLinkedService,\r\n",
        "  \"spark.cosmos.container\": \"GreenTaxiRecordsCFSink\",\r\n",
        "  \"spark.cosmos.write.strategy\": \"ItemOverwrite\",\r\n",
        "  \"spark.cosmos.write.bulk.enabled\": \"true\",\r\n",
        "  \"spark.cosmos.throughputControl.enabled\": \"true\",\r\n",
        "  \"spark.cosmos.throughputControl.name\": \"NYCGreenTaxiDataIngestionSlink\",\r\n",
        "  \"spark.cosmos.throughputControl.targetThroughputThreshold\": \"0.90\",\r\n",
        "  \"spark.cosmos.throughputControl.globalControl.database\": \"SampleDatabase\",\r\n",
        "  \"spark.cosmos.throughputControl.globalControl.container\": \"ThroughputControl\",\r\n",
        "}\r\n",
        "\r\n",
        "df_NYCGreenTaxi_Input.write\\\r\n",
        "            .format(\"cosmos.oltp\")\\\r\n",
        "            .mode('append')\\\r\n",
        "            .options(**SlinkwriteCfg) \\\r\n",
        "            .save()\r\n",
        "\r\n",
        "print(\"Finished ingestion via Linked Service: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "import uuid\r\n",
        "import datetime\r\n",
        "\r\n",
        "print(\"Starting ingestion via Linked Service without Throughput Control: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "\r\n",
        "df_NYCGreenTaxi_Input = spark.sql('SELECT * FROM source')\r\n",
        "\r\n",
        "SlinkNoThCtrlwriteCfg = {\r\n",
        "  \"spark.synapse.linkedService\": cosmosLinkedService,\r\n",
        "  \"spark.cosmos.container\": \"GreenTaxiRecordsMAXThroughput\",\r\n",
        "  \"spark.cosmos.write.strategy\": \"ItemOverwrite\",\r\n",
        "  \"spark.cosmos.write.bulk.enabled\": \"true\"\r\n",
        "}\r\n",
        "\r\n",
        "df_NYCGreenTaxi_Input.write\\\r\n",
        "            .format(\"cosmos.oltp\")\\\r\n",
        "            .mode('append')\\\r\n",
        "            .options(**SlinkNoThCtrlwriteCfg) \\\r\n",
        "            .save()\r\n",
        "\r\n",
        "print(\"Finished ingestion via Linked Service without Throughput Control: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting the reference record count**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a84d335d-b0f1-480f-8826-07b66ad1f1d8"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_source = spark.sql('SELECT * FROM source').count()\n",
        "print(\"Number of records in source: \", count_source) "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "dd176767-080a-4755-8cfd-4d89602b7c5c"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - validating the record count via query**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4f8b1a3b-44ed-4117-aae6-09c85bb86981"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "print(\"Starting validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
        "readCfg = {\n",
        "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n",
        "  \"spark.cosmos.accountKey\": cosmosMasterKey,\n",
        "  \"spark.cosmos.database\": \"SampleDatabase\",\n",
        "  \"spark.cosmos.container\": \"GreenTaxiRecords\",\n",
        "  \"spark.cosmos.read.partitioning.strategy\": \"Restrictive\",#IMPORTANT - any other partitioning strategy will result in indexing not being use to count - so latency and RU would spike up\n",
        "  \"spark.cosmos.read.inferSchema.enabled\" : \"false\",\n",
        "  \"spark.cosmos.read.customQuery\" : \"SELECT COUNT(0) AS Count FROM c\"\n",
        "}\n",
        "\n",
        "count_query_schema=StructType(fields=[StructField(\"Count\", LongType(), True)])\n",
        "query_df = spark.read.format(\"cosmos.oltp\").schema(count_query_schema).options(**readCfg).load()\n",
        "count_query = query_df.select(F.sum(\"Count\").alias(\"TotalCount\")).first()[\"TotalCount\"]\n",
        "print(\"Number of records retrieved via query: \", count_query) \n",
        "print(\"Finished validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
        "\n",
        "assert count_source == count_query"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6c56441d-c641-4751-99af-fcd9461da033"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To query Cosmos Analytical Store containers using ```spark.read.format(\"cosmos.olap\")``` we would need to use a modified version of the above example based on the following documentation [reference](https://docs.microsoft.com/en-us/azure/synapse-analytics/synapse-link/how-to-query-analytical-store-spark-3)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\r\n",
        "import pyspark.sql.functions as F\r\n",
        "\r\n",
        "print(\"Starting validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "readCfg = {\r\n",
        "  \"spark.synapse.linkedService\": cosmosLinkedService,\r\n",
        "  \"spark.cosmos.container\": \"GreenTaxiRecords\"\r\n",
        "}\r\n",
        "\r\n",
        "query_df = spark.read.format(\"cosmos.olap\").options(**readCfg).load()\r\n",
        "count_query = query_df.count()\r\n",
        "print(\"Number of records retrieved via query: \", count_query) \r\n",
        "print(\"Finished validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "\r\n",
        "assert count_source == count_query"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - validating the record count via change feed**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fc07b9a6-5021-49cb-acb6-1c345caab955"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting validation via change feed: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
        "changeFeedCfg = {\n",
        "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n",
        "  \"spark.cosmos.accountKey\": cosmosMasterKey,\n",
        "  \"spark.cosmos.database\": \"SampleDatabase\",\n",
        "  \"spark.cosmos.container\": \"GreenTaxiRecordsCFSink\",\n",
        "  \"spark.cosmos.read.partitioning.strategy\": \"Default\",\n",
        "  \"spark.cosmos.read.inferSchema.enabled\" : \"false\",\n",
        "  \"spark.cosmos.changeFeed.startFrom\" : \"Beginning\",\n",
        "  \"spark.cosmos.changeFeed.mode\" : \"Incremental\"\n",
        "}\n",
        "changeFeed_df = spark.read.format(\"cosmos.oltp.changeFeed\").options(**changeFeedCfg).load()\n",
        "count_changeFeed = changeFeed_df.count()\n",
        "print(\"Number of records retrieved via change feed: \", count_changeFeed) \n",
        "print(\"Finished validation via change feed: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
        "\n",
        "assert count_source == count_changeFeed"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e02a1e59-1dcd-4e9d-b8b3-20028798f710"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - bulk deleting documents and validating document count afterwards**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f43c9e10-95ec-4059-a764-2268472877fd"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "New and more straight forward way to bulk delete documents with new ``[\"spark.cosmos.write.strategy\"] = \"ItemDelete\"``"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "print(\"Starting to identify to be deleted documents: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
        "readCfg = {\n",
        "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n",
        "  \"spark.cosmos.accountKey\": cosmosMasterKey,\n",
        "  \"spark.cosmos.database\": \"SampleDatabase\",\n",
        "  \"spark.cosmos.container\": \"GreenTaxiRecords\",\n",
        "  \"spark.cosmos.read.partitioning.strategy\": \"Default\",\n",
        "  \"spark.cosmos.read.inferSchema.enabled\" : \"false\",\n",
        "}\n",
        "\n",
        "toBeDeleted_df = spark.read.format(\"cosmos.oltp\").options(**readCfg).load().limit(100_000)\n",
        "print(\"Number of records to be deleted: \", toBeDeleted_df.count()) \n",
        "\n",
        "print(\"Starting to bulk delete documents: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
        "deleteCfg = writeCfg.copy()\n",
        "deleteCfg[\"spark.cosmos.write.strategy\"] = \"ItemDelete\"\n",
        "toBeDeleted_df \\\n",
        "        .write \\\n",
        "        .format(\"cosmos.oltp\") \\\n",
        "        .mode(\"Append\") \\\n",
        "        .options(**deleteCfg) \\\n",
        "        .save()\n",
        "print(\"Finished deleting documents: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
        "\n",
        "print(\"Starting count validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
        "count_query_schema=StructType(fields=[StructField(\"Count\", LongType(), True)])\n",
        "query_df = spark.read.format(\"cosmos.oltp\").schema(count_query_schema).options(**readCfg).load()\n",
        "count_query = query_df.select(F.sum(\"Count\").alias(\"TotalCount\")).first()[\"TotalCount\"]\n",
        "print(\"Number of records retrieved via query: \", count_query) \n",
        "print(\"Finished count validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
        "\n",
        "#assert max(0, count_source - 100_000) == count_query"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "63be3f23-ae76-4813-8bdb-40733e56e087"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample of BulkDelete with Throughput Control options** \r\n",
        "\r\n",
        "This example will build on top of previous example to implement throughput control for both reading the source data and BulkDelete"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Throughput controlled delete\r\n",
        "import math\r\n",
        "\r\n",
        "print(\"Starting to identify to be deleted documents with Throughput Control: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "readCfg = {\r\n",
        "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\r\n",
        "  \"spark.cosmos.accountKey\": cosmosMasterKey,\r\n",
        "  \"spark.cosmos.database\": \"SampleDatabase\",\r\n",
        "  \"spark.cosmos.container\": \"GreenTaxiRecordsCFSink\",\r\n",
        "  \"spark.cosmos.read.partitioning.strategy\": \"Default\",\r\n",
        "  \"spark.cosmos.read.inferSchema.enabled\" : \"false\",\r\n",
        "  \"spark.cosmos.throughputControl.enabled\": \"true\",\r\n",
        "  \"spark.cosmos.throughputControl.name\": \"NYCGreenTaxiDataDelete2\",\r\n",
        "  \"spark.cosmos.throughputControl.targetThroughputThreshold\": \"0.50\",\r\n",
        "  \"spark.cosmos.throughputControl.globalControl.database\": \"SampleDatabase\",\r\n",
        "  \"spark.cosmos.throughputControl.globalControl.container\": \"ThroughputControl\"\r\n",
        "}\r\n",
        "\r\n",
        "total_df = spark.read.format(\"cosmos.oltp\").options(**readCfg).load()\r\n",
        "print(\"Original Number of records: \", total_df.count()) \r\n",
        "\r\n",
        "toBeDeleted_df = spark.read.format(\"cosmos.oltp\").options(**readCfg).load().limit(100_000)\r\n",
        "print(\"Number of records to be deleted: \", toBeDeleted_df.count()) \r\n",
        "\r\n",
        "print(\"Starting to bulk delete documents with Throughput Control: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "\r\n",
        "deleteCfg = {\r\n",
        "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\r\n",
        "  \"spark.cosmos.accountKey\": cosmosMasterKey,\r\n",
        "  \"spark.cosmos.database\": cosmosDatabase,\r\n",
        "  \"spark.cosmos.container\": \"GreenTaxiRecordsCFSink\",\r\n",
        "  \"spark.cosmos.write.strategy\": \"ItemDelete\",\r\n",
        "  \"spark.cosmos.write.bulk.enabled\": \"true\",\r\n",
        "  \"spark.cosmos.throughputControl.enabled\": \"true\",\r\n",
        "  \"spark.cosmos.throughputControl.name\": \"NYCGreenTaxiDataDelete2\",\r\n",
        "  \"spark.cosmos.throughputControl.targetThroughputThreshold\": \"0.50\",\r\n",
        "  \"spark.cosmos.throughputControl.globalControl.database\": \"SampleDatabase\",\r\n",
        "  \"spark.cosmos.throughputControl.globalControl.container\": \"ThroughputControl\"\r\n",
        "}\r\n",
        "\r\n",
        "toBeDeleted_df \\\r\n",
        "        .write \\\r\n",
        "        .format(\"cosmos.oltp\") \\\r\n",
        "        .mode(\"Append\") \\\r\n",
        "        .options(**deleteCfg) \\\r\n",
        "        .save()\r\n",
        "print(\"Finished deleting documents with Throughput Control: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "\r\n",
        "print(\"Starting count validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "count_query_schema=StructType(fields=[StructField(\"Count\", LongType(), True)])\r\n",
        "query_df = spark.read.format(\"cosmos.oltp\").schema(count_query_schema).options(**readCfg).load()\r\n",
        "count_query = query_df.select(F.sum(\"Count\").alias(\"TotalCount\")).first()[\"TotalCount\"]\r\n",
        "print(\"Number of records retrieved via query: \", count_query) \r\n",
        "print(\"Finished count validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "\r\n",
        "#assert max(0, count_source - 100_000) == count_query\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample of BulkDelete with Throughput Control options and using OLAP for reads** \r\n",
        "\r\n",
        "This example will build on top of previous example to use Cosmos Analytical Store/OLAP connector to read data and OLTP for BulkDelete"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Throughput controlled delete with reads using OLAP\r\n",
        "import math\r\n",
        "\r\n",
        "print(\"Starting to identify to be deleted documents with Throughput Control and Analytical store: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "readCfg = {\r\n",
        "  \"spark.synapse.linkedService\": cosmosLinkedService,\r\n",
        "  \"spark.cosmos.container\": \"GreenTaxiRecordsCFSink\"\r\n",
        "}\r\n",
        "\r\n",
        "total_df = spark.read.format(\"cosmos.olap\").options(**readCfg).load()\r\n",
        "print(\"Original Number of records: \", total_df.count()) \r\n",
        "\r\n",
        "toBeDeleted_df = spark.read.format(\"cosmos.olap\").options(**readCfg).load().limit(100_000)\r\n",
        "print(\"Number of records to be deleted: \", toBeDeleted_df.count()) \r\n",
        "\r\n",
        "print(\"Starting to bulk delete documents with Throughput Control and Analytical store: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "\r\n",
        "deleteCfg = {\r\n",
        "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\r\n",
        "  \"spark.cosmos.accountKey\": cosmosMasterKey,\r\n",
        "  \"spark.cosmos.database\": cosmosDatabase,\r\n",
        "  \"spark.cosmos.container\": \"GreenTaxiRecordsCFSink\",\r\n",
        "  \"spark.cosmos.write.strategy\": \"ItemDelete\",\r\n",
        "  \"spark.cosmos.write.bulk.enabled\": \"true\",\r\n",
        "  \"spark.cosmos.throughputControl.enabled\": \"true\",\r\n",
        "  \"spark.cosmos.throughputControl.name\": \"NYCGreenTaxiDataDelete3\",\r\n",
        "  \"spark.cosmos.throughputControl.targetThroughputThreshold\": \"0.50\",\r\n",
        "  \"spark.cosmos.throughputControl.globalControl.database\": \"SampleDatabase\",\r\n",
        "  \"spark.cosmos.throughputControl.globalControl.container\": \"ThroughputControl\"\r\n",
        "}\r\n",
        "\r\n",
        "toBeDeleted_df \\\r\n",
        "        .write \\\r\n",
        "        .format(\"cosmos.oltp\") \\\r\n",
        "        .mode(\"Append\") \\\r\n",
        "        .options(**deleteCfg) \\\r\n",
        "        .save()\r\n",
        "print(\"Finished deleting documents with Throughput Control and Analytical store: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "\r\n",
        "\r\n",
        "#assert max(0, count_source - 100_000) == count_query\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting count validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n",
        "\r\n",
        "query_df = spark.read.format(\"cosmos.olap\").options(**readCfg).load()\r\n",
        "count_query = query_df.count()\r\n",
        "print(\"Number of records retrieved via query of replicated Analytical Store: \", count_query) \r\n",
        "print(\"Finished count validation via query: \", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - showing the existing Containers**\r\n",
        "\r\n",
        "Below examples show new Catalog API capabilities"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "907f3933-5008-45d1-bba6-a426ca13c823"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "SHOW TABLES FROM cosmosCatalog.SampleDatabase"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "883d1de2-7fdc-45f8-a1e3-1cf53c3b9855"
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Tables = spark.sql('SHOW TABLES FROM cosmosCatalog.SampleDatabase')\n",
        "#assert df_Tables.count() == 3"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b027d779-b88b-4dab-bc0d-61e3fb35d2ce"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - querying a Cosmos Container via Spark Catalog**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "10d6d127-7db3-479b-aeef-c45d595a3885"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "SELECT * FROM cosmosCatalog.SampleDatabase.GreenTaxiRecords LIMIT 10"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "5b9e59a7-5318-4a6f-b9a8-60e89c6627d6"
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample - querying a Cosmos Container with custom settings via Spark Catalog**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "456b891c-e973-4bf9-97f0-a9151ed93196"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the view with custom settings (in this case adding a projection, disabling schema inference and switching to aggressive partitioning strategy)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a321f36f-c1f6-4b2b-aa79-5cf00f56bf3e"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "CREATE TABLE cosmosCatalog.SampleDatabase.GreenTaxiRecordsView \n",
        "  (id STRING, _ts TIMESTAMP, vendorID INT, totalAmount DOUBLE)\n",
        "USING cosmos.oltp\n",
        "TBLPROPERTIES(isCosmosView = 'True')\n",
        "OPTIONS (\n",
        "  spark.cosmos.database = 'SampleDatabase',\n",
        "  spark.cosmos.container = 'GreenTaxiRecords',\n",
        "  spark.cosmos.read.inferSchema.enabled = 'False',\n",
        "  spark.cosmos.read.inferSchema.includeSystemProperties = 'True',\n",
        "  spark.cosmos.read.partitioning.strategy = 'Aggressive');\n",
        "\n",
        "SELECT * FROM cosmosCatalog.SampleDatabase.GreenTaxiRecordsView LIMIT 10"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4830781a-2c67-4a1d-bb0d-a64655142fa1"
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating another view with custom settings (in this case enabling schema inference and switching to restrictive partitioning strategy)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "800d868e-1d32-4eee-8a1d-0c947d0af715"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "CREATE TABLE cosmosCatalog.SampleDatabase.GreenTaxiRecordsAnotherView \n",
        "USING cosmos.oltp\n",
        "TBLPROPERTIES(isCosmosView = 'True')\n",
        "OPTIONS (\n",
        "  spark.cosmos.database = 'SampleDatabase',\n",
        "  spark.cosmos.container = 'GreenTaxiRecords',\n",
        "  spark.cosmos.read.inferSchema.enabled = 'True',\n",
        "  spark.cosmos.read.inferSchema.includeSystemProperties = 'False',\n",
        "  spark.cosmos.read.partitioning.strategy = 'Restrictive');\n",
        "\n",
        "SELECT * FROM cosmosCatalog.SampleDatabase.GreenTaxiRecordsAnotherView LIMIT 10"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cea14de6-4be9-4cb7-b6d2-00928a75f912"
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show all Tables in the Cosmos Catalog to show that both the \"real\" Containers as well as the views show-up"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "23cc237d-251e-4f31-a73f-4e2786316eaf"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "SHOW TABLES FROM cosmosCatalog.SampleDatabase"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2ad2e367-b5d8-42b3-8a56-2505869eae6b"
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Tables = spark.sql('SHOW TABLES FROM cosmosCatalog.SampleDatabase')\n",
        "#assert df_Tables.count() == 5"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cb0df38d-27ef-462d-8181-8df3d676e86a"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cleanup the views again**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "865ccc18-5342-4de2-969b-1e0670272797"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "DROP TABLE IF EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecordsView;\n",
        "DROP TABLE IF EXISTS cosmosCatalog.SampleDatabase.GreenTaxiRecordsAnotherView;\n",
        "SHOW TABLES FROM cosmosCatalog.SampleDatabase"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1ff1f132-8970-4a33-ac84-241df8e4f57f"
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Tables = spark.sql('SHOW TABLES FROM cosmosCatalog.SampleDatabase')\n",
        "#assert df_Tables.count() == 3"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "85142b5a-c14b-4b93-bf9e-2bc7f9e2c3f5"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}